{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e83ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "### local DB setup and load\n",
    "import os\n",
    "import io\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import fitz \n",
    "import pymupdf\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Users\\\\yifyun01\\AppData\\\\Local\\Programs\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "\n",
    "print(load_dotenv(\"../chatbot_poc_playground/.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a8968",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0b5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_one_pdf(file_path, ocr_flag=True):\n",
    "\n",
    "    all_chunks = []            \n",
    "    \n",
    "    if ocr_flag:\n",
    "        chunks = load_pdf_ocr_file(file_path)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    else:\n",
    "        pages = load_pdf_file(file_path)\n",
    "        chunks = split_pdf_pages_into_chunks(pdf_pages=pages, \n",
    "                                    chunk_size=6000, \n",
    "                                    chunk_overlap=300)\n",
    "        all_chunks.extend(chunks) \n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "\n",
    "def load_and_chunk_pdfs(pdf_dir, ocr_flag=True):\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.lower().endswith(\"pdf\"):\n",
    "            file_path = os.path.join(pdf_dir, filename)            \n",
    "            print(f\"Loading file: {filename}\")\n",
    "            \n",
    "            if ocr_flag:\n",
    "                chunks = load_pdf_ocr_file(file_path)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            else:\n",
    "                pages = load_pdf_file(file_path)\n",
    "                chunks = split_pdf_pages_into_chunks(pdf_pages=pages, \n",
    "                                            chunk_size=6000, \n",
    "                                            chunk_overlap=300)\n",
    "                all_chunks.extend(chunks) \n",
    "            \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "\n",
    "def load_pdf_ocr_file(file_path): \n",
    "      \n",
    "    doc = pymupdf.open(file_path)\n",
    "\n",
    "    chunk_list = []\n",
    "\n",
    "    for page_num in range(len(doc)):       \n",
    "        page = doc[page_num]\n",
    "        # Render page to a pixmap (image)\n",
    "        pix = page.get_pixmap(dpi=300)  \n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        # Use PIL to open image\n",
    "        image = Image.open(io.BytesIO(img_data))\n",
    "        # Run OCR\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        \n",
    "        chunk_list.extend([text])\n",
    "          \n",
    "    return chunk_list\n",
    "\n",
    "\n",
    "def load_pdf_file(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # Load pages\n",
    "    pages = loader.load()\n",
    "    return pages\n",
    "\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Read entire file as a single string\n",
    "        content = f.read()  \n",
    "    return content.strip()\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=5000, chunk_overlap=300):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def split_pdf_pages_into_chunks(pdf_pages, chunk_size=5000, chunk_overlap=300):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    chunks = splitter.split_documents(pdf_pages)    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract JSON array from text that might contain additional content.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that may contain JSON\n",
    "        \n",
    "    Returns:\n",
    "        The parsed JSON if found, None otherwise\n",
    "    \"\"\"\n",
    "    # First, check if the text is wrapped in code blocks with triple backticks\n",
    "    code_block_pattern = r'```(?:json)?\\s*([\\s\\S]*?)```'\n",
    "    code_match = re.search(code_block_pattern, text)\n",
    "    if code_match:\n",
    "        text = code_match.group(1).strip()\n",
    "        print(\"Found JSON in code block, extracting content...\")\n",
    "    \n",
    "    try:\n",
    "        # Try direct parsing in case the response is already clean JSON\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Look for opening and closing brackets of a JSON array\n",
    "        start_idx = text.find('[')\n",
    "        if start_idx == -1:\n",
    "            print(\"No JSON array start found in text\")\n",
    "            return None\n",
    "            \n",
    "        # Simple bracket counting to find matching closing bracket\n",
    "        bracket_count = 0\n",
    "        complete_json = False\n",
    "        for i in range(start_idx, len(text)):\n",
    "            if text[i] == '[':\n",
    "                bracket_count += 1\n",
    "            elif text[i] == ']':\n",
    "                bracket_count -= 1\n",
    "                if bracket_count == 0:\n",
    "                    # Found the matching closing bracket\n",
    "                    json_str = text[start_idx:i+1]\n",
    "                    complete_json = True\n",
    "                    break\n",
    "        \n",
    "        # Handle complete JSON array\n",
    "        if complete_json:\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Found JSON-like structure but couldn't parse it.\")\n",
    "                print(\"Trying to fix common formatting issues...\")\n",
    "                \n",
    "                # Try to fix missing quotes around keys\n",
    "                fixed_json = re.sub(r'(\\s*)(\\w+)(\\s*):(\\s*)', r'\\1\"\\2\"\\3:\\4', json_str)\n",
    "                # Fix trailing commas\n",
    "                fixed_json = re.sub(r',(\\s*[\\]}])', r'\\1', fixed_json)\n",
    "                \n",
    "                try:\n",
    "                    return json.loads(fixed_json)\n",
    "                except:\n",
    "                    print(\"Could not fix JSON format issues\")\n",
    "        else:\n",
    "            # Handle incomplete JSON - try to complete it\n",
    "            print(\"Found incomplete JSON array, attempting to complete it...\")\n",
    "            \n",
    "            # Get all complete objects from the array\n",
    "            objects = []\n",
    "            obj_start = -1\n",
    "            obj_end = -1\n",
    "            brace_count = 0\n",
    "            \n",
    "            # First find all complete objects\n",
    "            for i in range(start_idx + 1, len(text)):\n",
    "                if text[i] == '{':\n",
    "                    if brace_count == 0:\n",
    "                        obj_start = i\n",
    "                    brace_count += 1\n",
    "                elif text[i] == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        obj_end = i\n",
    "                        objects.append(text[obj_start:obj_end+1])\n",
    "            \n",
    "            if objects:\n",
    "                # Reconstruct a valid JSON array with complete objects\n",
    "                reconstructed_json = \"[\\n\" + \",\\n\".join(objects) + \"\\n]\"\n",
    "                try:\n",
    "                    return json.loads(reconstructed_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Couldn't parse reconstructed JSON array.\")\n",
    "                    print(\"Trying to fix common formatting issues...\")\n",
    "                    \n",
    "                    # Try to fix missing quotes around keys\n",
    "                    fixed_json = re.sub(r'(\\s*)(\\w+)(\\s*):(\\s*)', r'\\1\"\\2\"\\3:\\4', reconstructed_json)\n",
    "                    # Fix trailing commas\n",
    "                    fixed_json = re.sub(r',(\\s*[\\]}])', r'\\1', fixed_json)\n",
    "                    \n",
    "                    try:\n",
    "                        return json.loads(fixed_json)\n",
    "                    except:\n",
    "                        print(\"Could not fix JSON format issues in reconstructed array\")\n",
    "            \n",
    "        print(\"No complete JSON array could be extracted\")\n",
    "        return None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fa365",
   "metadata": {},
   "source": [
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bb6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other? They don't have the guts.\n"
     ]
    }
   ],
   "source": [
    "### Setup Azure OpenAI model instance\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    openai_api_version =os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),    \n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"tell me a joke\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "\n",
    "# Load one pdf to test prompt\n",
    "pdf_path = \"./contracts/LinkedIn.pdf\"\n",
    "all_chunks = load_and_chunk_one_pdf(pdf_path)\n",
    "input_text_for_entity_extract = \"\".join(all_chunks[:5])\n",
    "input_text_all = all_chunks\n",
    "# print(input_text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a563d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts_test import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "prompt_entity_extraction = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", MAIN_SYSTEM_PROMPT),\n",
    "    (\"user\", MAIN_USER_PROMPT_V2)\n",
    "])\n",
    "\n",
    "prompt_high_risk_clauses = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", MAIN_SYSTEM_PROMPT),\n",
    "    (\"user\", MAIN_USER_PROMPT_HIGH_RISK_CLAUSE_V2)\n",
    "])\n",
    "\n",
    "\n",
    "chain_entity_extraction = prompt_entity_extraction | llm | JsonOutputParser()\n",
    "\n",
    "chain_high_risk_clauses = prompt_high_risk_clauses | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5325a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primaryEntity': {'id': 'pe-001',\n",
       "  'name': 'Robert Half Inc.',\n",
       "  'description': 'Parent company',\n",
       "  'counterparties': [{'id': 'cp-001',\n",
       "    'name': 'LinkedIn Corporation',\n",
       "    'description': 'Professional networking platform',\n",
       "    'contractDates': {'effectiveDate': 'N/A', 'endDate': 'N/A'},\n",
       "    'services': [{'id': 'svc-001',\n",
       "      'name': 'Premium Services',\n",
       "      'description': 'Services specified in Order Forms for recruiting and contacting prospective clients'}]}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_entity_extraction.invoke({\"input_text\": input_text_for_entity_extract })\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d38de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/linkedin_entity.json\", \"w\") as f:\n",
    "    json.dump(response, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31ad6c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high-risk clauses': ['LinkedIn reserves the right to modify the User Agreement at any time for any reason. Notice of any change will be sent to the Customer in writing and made available via the Linkedin website. If the Parties agree that such modifications materially degrade the ability of the Customer to use the Premium Services, Customer may terminate the applicable Order Form within thirty (30) days after such change and receive a refund of any unused Fee for Premium Services.',\n",
       "  \"Customer agrees to pay all undisputed fees for the Premium Services included in the applicable Order Form ('Service Fees'), even if Users do not activate or use the Premium Services.\",\n",
       "  'Customer will not allow the Premium Services to be used as a service bureau for third parties (i.e. Customer will not provide or sell access to the Premium Services to third parties).',\n",
       "  'Linkedin reserves the right to suspend the Premium Services for an individual Customer User account with or without notice to Customer, if Linkedin reasonably suspects such Customer User is in material breach of the material terms and conditions of this Agreement.',\n",
       "  'Upon termination or expiration of this Agreement or any Order Form (a) except for the right to undisputed accrued Fees, all rights granted by Linkedin herein will terminate immediately; (b) any and all payment obligations will be due, as provided in Section 3 (Fees and Payments) and the applicable Order Form; (c) Customer will notify Customer’s Users that their access to the Premium Services has terminated, and (d) Linkedin may withhold, remove or discard any content posted by Customer in the Premium Services.',\n",
       "  \"UNLESS SPECIFIED OTHERWISE, LINKEDIN DOES NOT AND CANNOT WARRANT THAT THE PREMIUM SERVICES WILL BE UNINTERRUPTED OR ERROR-FREE. CUSTOMER ACKNOWLEDGES AND AGREES THAT THE PREMIUM SERVICES ARE PROVIDED 'AS IS' AND THAT, EXCEPT AS EXPRESSLY PROVIDED IN THIS AGREEMENT, LINKEDIN MAKES NO GUARANTEES, CONDITIONS, COVENANTS, REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.\",\n",
       "  \"EXCEPT WITH RESPECT TO OBLIGATIONS UNDER SECTION 5 (CONFIDENTIAL INFORMATION), SECTION 9 (THIRD PARTY INDEMNITY) OR VIOLATION OF EITHER PARTY'S INTELLECTUAL PROPERTY RIGHTS, NEITHER PARTY WILL BE LIABLE TO THE OTHER FOR ANY INCIDENTAL, CONSEQUENTIAL, INDIRECT, PUNITIVE, SPECIAL OR RELIANCE DAMAGES RELATED TO THIS AGREEMENT OR THE SERVICES PROVIDED HERE\"]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_high_risk_clauses = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", MAIN_SYSTEM_PROMPT),\n",
    "    (\"user\", MAIN_USER_PROMPT_HIGH_RISK_CLAUSE_V2)\n",
    "])\n",
    "\n",
    "chain_high_risk_clauses = prompt_high_risk_clauses | llm | JsonOutputParser()\n",
    "\n",
    "response = chain_high_risk_clauses.invoke({\"input_text\": input_text_for_entity_extract })\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52d67801",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/linkedin_high_risk_clauses.json\", \"w\") as f:\n",
    "    json.dump(response, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950bdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_stream):\n",
    "    \"\"\"Extract text from an in-memory PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pymupdf.open(stream=pdf_stream, filetype=\"pdf\") as pdf:\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_entity(selected_file):    \n",
    "    \n",
    "    all_chunks = load_and_chunk_one_pdf(f\"./contracts/{selected_file}\")\n",
    "    input_text_for_entity_extract = \"\".join(all_chunks[:5])\n",
    "    \n",
    "    resp = llm.invoke(prompt_entity_extraction.format_prompt(input_text=input_text_for_entity_extract))\n",
    "    summary = resp.content\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_clauses(selected_file):    \n",
    "    \n",
    "    all_chunks = load_and_chunk_one_pdf(f\"./contracts/{selected_file}\")\n",
    "    input_text_for_entity_extract = \"\".join(all_chunks[:10])\n",
    "    \n",
    "    resp = llm.invoke(prompt_high_risk_clauses.format_prompt(input_text=input_text_for_entity_extract))\n",
    "    summary = resp.content\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647b80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#===============#\n",
    "# Create the UI #\n",
    "#===============#\n",
    "import gradio as gr\n",
    "\n",
    "pdf_dir = \"./contracts\"\n",
    "file_list = [file_name for file_name in os.listdir(pdf_dir) if file_name.lower().endswith(\"pdf\")]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # gr.Markdown('### PDF Summarization AI')\n",
    "    gr.HTML(\"<center><h1>Third-Party Contract AI</h1></center>\")\n",
    "    with gr.Row():\n",
    "         \n",
    "        dropdown = gr.Dropdown(choices = file_list, label=\"Select a PDF File\")\n",
    "        entity_button = gr.Button(\"Extract key entities For Me\", variant='primary')\n",
    "        clauses_button = gr.Button(\"Extract high-risk clauses For Me\", variant='primary')\n",
    "    \n",
    "    output_entity_box = gr.Textbox(label=\"Key Contract Entities\", lines =10)\n",
    "    output_clauses_box = gr.Textbox(label=\"High-risk Clauses\", lines =10)\n",
    "\n",
    "    entity_button.click(fn=generate_entity, inputs=dropdown, outputs=output_entity_box)\n",
    "    clauses_button.click(fn=generate_clauses, inputs=dropdown, outputs=output_clauses_box)\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c294425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10ad19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14da80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e56f0df6",
   "metadata": {},
   "source": [
    "### Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities\n",
    "system_prompt = MAIN_SYSTEM_PROMPT\n",
    "user_prompt = MAIN_USER_PROMPT_V1\n",
    "user_prompt += f\"```\\n{input_text_for_entity_extract}```\\n\" \n",
    "\n",
    "response = llm.invoke(system_prompt + user_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract high-risk clauses\n",
    "\n",
    "pdf_path = \"./contracts/UIPath.pdf\"\n",
    "# all_chunks = load_and_chunk_pdfs(pdf_path, ocr_flag=True)\n",
    "# all_chunks = load_and_chunk_one_pdf(pdf_path)\n",
    "print(f\"Total contract chunks: {len(all_chunks)}\")\n",
    "\n",
    "# Process each chunk\n",
    "system_prompt = MAIN_SYSTEM_PROMPT\n",
    "user_prompt = MAIN_USER_PROMPT_HIGH_RISK_CLAUSE_V1\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(all_chunks)}\")\n",
    "    \n",
    "    # Process the chunk with LLM\n",
    "    chunk_response = llm.invoke(system_prompt + user_prompt + f\"```\\n{chunk}```\\n\" )\n",
    "    chunk_results = chunk_response.content\n",
    "    \n",
    "    if chunk_results:              \n",
    "        # Add to overall results\n",
    "        all_results.extend([chunk_results])\n",
    "    else:\n",
    "        print(f\"Warning: Failed to extract triples from chunk {i+1}\")\n",
    "\n",
    "print(f\"\\nExtracted a total of {len(all_results)} triples from all chunks\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Your raw nested list (as strings)\n",
    "# Flattening logic\n",
    "flattened_list = []\n",
    "for s in all_results:\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)  # safely parse string to list\n",
    "        flattened_list.extend(parsed)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping one item due to parsing error: {e}\")\n",
    "\n",
    "# Output flattened list\n",
    "for item in flattened_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_txt_clauses = {\"high-risk clauses\": flattened_list}\n",
    "\n",
    "with open(\"high_risk_clauses.json\", \"w\") as f:\n",
    "    json.dump(json_txt_clauses, f, indent=4)\n",
    "\n",
    "# df = pd.DataFrame(data=all_results, columns=['json_outpt'])\n",
    "# df.to_excel(\"extraction.xlsx\")\n",
    "# print(df['json_outpt'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf8b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d00ade8",
   "metadata": {},
   "source": [
    "### Parse 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Company info\n",
    "ticker = \"MSFT\"  # Apple Inc.\n",
    "cik_lookup_url = f\"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "# Headers (SEC requires a User-Agent)\n",
    "headers = {\n",
    "    \"User-Agent\": \"yifei.yun@protiviti.com\"\n",
    "}\n",
    "\n",
    "# Step 1: Get CIK from ticker\n",
    "response = requests.get(cik_lookup_url, headers=headers)\n",
    "ticker_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d65fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# # Create a downloader instance\n",
    "# dl = Downloader(\"Protiviti\", \"yifei.yun@protiviti.com\", \"./data\")\n",
    "# # Download the latest 10-K (1 means the most recent)\n",
    "# ticker = \"MSFT\"\n",
    "# dl.get(\"10-K\", ticker, limit=1)\n",
    "\n",
    "\n",
    "# Path to downloaded file (find the first .htm file)\n",
    "html_file = \"./data/MSFT/10-K.html\"\n",
    "# filename = [f for f in os.listdir(folder_path) if f.endswith(\".htm\") or f.endswith(\".html\")][0]\n",
    "\n",
    "# Extract text content\n",
    "def html_to_clean_text(html_path):\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"lxml\")\n",
    "\n",
    "    # Remove non-visible elements\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"meta\", \"noscript\", \"footer\", \"nav\", \"tr\", \"td\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Get text content\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # Optional: collapse excessive whitespace\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    clean_lines = [line for line in lines if line]  # remove empty lines\n",
    "    return \"\\n\".join(clean_lines)\n",
    "\n",
    "clean_text = html_to_clean_text(html_file)\n",
    "\n",
    "with open(\"./data/MSFT/clean_10k_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba6cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274ca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9c9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbae25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
